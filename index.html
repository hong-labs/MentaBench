<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>MentaBench: A Multi-Task Benchmark for Mobile Mental Health Prediction with Small Language Models on Social Media Posts</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description" content="MentaBench: A Multi-Task Benchmark for Mobile Mental Health Prediction with Small Language Models on Social Media Posts" />
  <style>
    :root {
      --bg-from: #d8f1ea;
      --bg-to:   #c0e4df;
      --text-main: #111827;
      --text-sub:  #4b5563;
      --card-bg:   #ffffff;
      --border:    #e5e7eb;
      --max-width: 960px;

      --btn-bg:    #c9ded7;
      --btn-border:#90a6a0;
    }

    * {
      box-sizing: border-box;
      margin: 0;
      padding: 0;
    }

    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "SF Pro Text",
                   "Segoe UI", sans-serif;
      background: radial-gradient(circle at top left,
                  var(--bg-from) 0, var(--bg-to) 60%, #bcded7 100%);
      color: var(--text-main);
      line-height: 1.6;
    }

    a {
      color: inherit;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    /* ---------- HERO：顶部区域，仿 Web2Code ---------- */
    .hero {
      min-height: 260px;
      display: flex;
      flex-direction: column;
      align-items: center;
      text-align: center;
      padding: 60px 16px 40px;
    }

    .hero-inner {
      max-width: var(--max-width);
      width: 100%;
      margin: 0 auto;
    }

    h1.title {
      font-size: clamp(2.6rem, 4vw, 3.2rem);
      margin-bottom: 18px;
      font-weight: 700;
    }

    .authors {
      font-size: 0.95rem;
      color: var(--text-main);
      margin-bottom: 6px;
    }

    .affiliations {
      font-size: 0.85rem;
      color: var(--text-sub);
      margin-bottom: 28px;
    }

    .btn-row {
      display: flex;
      justify-content: center;
      flex-wrap: wrap;
      gap: 24px;
    }

    .link-btn {
      display: inline-flex;
      align-items: center;
      gap: 8px;
      padding: 12px 30px;
      min-width: 160px;
      justify-content: center;
      border-radius: 14px;
      background: var(--btn-bg);
      border: 1px solid var(--btn-border);
      box-shadow: 0 6px 14px rgba(15, 23, 42, 0.16);
      font-size: 1rem;
      color: #111827;
      transition:
        transform 0.15s ease,
        box-shadow 0.15s ease,
        background 0.15s ease,
        border-color 0.15s ease;
      text-decoration: none;
    }

    .link-btn:hover {
      transform: translateY(-2px);
      box-shadow: 0 10px 22px rgba(15, 23, 42, 0.22);
      background: #d6e9e3;
      border-color: #7f9992;
      text-decoration: none;
    }

    .link-icon {
      font-size: 1.3rem;
      display: inline-flex;
      align-items: center;
      justify-content: center;
      width: 1.4em;
    }

    /* ---------- 主体内容：下面的白色区域 ---------- */
    .content-wrapper {
      max-width: var(--max-width);
      margin: 0 auto 60px;
      padding: 0 16px 40px;
    }

    .content-card {
      background: var(--card-bg);
      border-radius: 24px 24px 24px 24px;
      padding: 28px 26px 32px;
      box-shadow: 0 -4px 20px rgba(15, 23, 42, 0.08);
      border: 1px solid rgba(148, 163, 184, 0.4);
    }

    @media (max-width: 640px) {
      .content-card {
        padding: 22px 16px 28px;
      }
    }

    .section {
      margin-bottom: 26px;
      text-align: left; 
    }

    .section:last-of-type {
      margin-bottom: 0;
    }

    .section h2 {
      font-size: 1.25rem;
      margin-bottom: 8px;
      text-align: left;
    }

    .section h3 {
      font-size: 1.05rem;
      margin: 10px 0 4px;
    }

    .section p {
      font-size: 0.96rem;
      color: var(--text-sub);
      text-align: justify;
      margin-bottom: 6px;
    }

    .section ul {
      font-size: 0.94rem;
      color: var(--text-sub);
      margin-left: 20px;
      margin-top: 4px;
    }

    .section li {
      margin-bottom: 4px;
    }

    .badge-row {
      display: flex;
      flex-wrap: wrap;
      gap: 6px;
      margin: 4px 0 6px;
    }

    .badge {
      font-size: 0.8rem;
      padding: 3px 8px;
      border-radius: 999px;
      background: #e0ecff;
      color: #1d4ed8;
      border: 1px solid rgba(129, 140, 248, 0.6);
    }

    .dataset-table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.85rem;
      margin-top: 6px;
    }

    .dataset-table th,
    .dataset-table td {
      border: 1px solid var(--border);
      padding: 4px 6px;
      text-align: left;
    }

    .dataset-table th {
      background: #f3f4ff;
      font-weight: 600;
    }

    .highlight {
      margin-top: 6px;
      font-size: 0.86rem;
      background: #e0f2fe;
      border-radius: 10px;
      padding: 6px 8px;
      color: #0f172a;
    }

    pre {
      background: #020617;
      color: #e5e7eb;
      padding: 10px 12px;
      border-radius: 12px;
      overflow-x: auto;
      font-size: 0.8rem;
      border: 1px solid rgba(30, 64, 175, 0.8);
    }

    code {
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas,
        "Liberation Mono", "Courier New", monospace;
    }

    footer {
      max-width: var(--max-width);
      margin: 0 auto 30px;
      padding: 10px 16px 0;
      font-size: 0.8rem;
      color: var(--text-sub);
      display: flex;
      justify-content: space-between;
      gap: 8px;
      flex-wrap: wrap;
    }

    html {
      scroll-behavior: smooth;
    }
    /* —— 三个手机截图排成一排 —— */
.device-row {
  max-width: 1000px;        /* 整块区域的最大宽度 */
  margin: 20px auto 0;      /* 居中 + 和上面内容留点空 */
  display: flex;
  flex-wrap: wrap;          /* 屏幕窄的时候自动换行 */
  justify-content: center;  /* 居中对齐 */
  gap: 20px;                /* 图片之间的间距 */
}

/* —— 单张手机截图的样式 —— */
.device-shot {
  display: block;
  width: 100%;
  max-width: 260px;         /* 每张图看起来多宽，可以改成 230 / 280 */
  border-radius: 26px;      /* 圆角，模拟手机壳边缘 */
  box-shadow: 0 20px 40px rgba(15, 23, 42, 0.35);  /* 投影 */
  background: #000;         /* 防止截图白边，看起来更扎实 */
}

/* 小屏幕时略微缩小一点 */
@media (max-width: 640px) {
  .device-shot {
    max-width: 220px;
  }
}
    /* 按钮里的小 logo 图标 */
.btn-icon-img {
  width: 20px;
  height: 20px;
  margin-right: 6px;
  display: inline-block;
  object-fit: contain;
  vertical-align: middle;
}
.authors sup {
  margin-left: 2px;        /* 名字和上标之间留一点空 */
  font-size: 0.75em;
}

.equal-note {
  display: block;
  margin-top: 4px;
  font-size: 0.85rem;
  color: var(--text-sub);
}
.method-list {
  max-width: 900px;        /* 可选：控制一行不要太长 */
  margin: 0 auto;          /* 让整个列表块在页面中居中，但内容左对齐 */
  padding-left: 1.5rem;
  text-align: left;
}

.method-list li {
  text-align: left !important;        /* 覆盖从父级继承的居中样式 */
}


  </style>
</head>
<body>

  <!-- ---------- HERO 顶部 ---------- -->
  <section class="hero">
  <div class="hero-inner">
    <h1 class="title">MentaBench: A Multi-Task Benchmark for Mobile Mental Health Prediction with Small Language Models on Social Media Posts</h1>

    <!-- 作者：名字 + * + 上标符号 -->
    <div class="authors">
      Tianyi Zhang<sup>†</sup>*,
      Xiangyuan Xue<sup>§</sup>*,
      Lingyan Ruan<sup>†</sup>,
      Yan Gao<sup>‡</sup>,
      Simon D'Alfonso<sup>†</sup>,
      Vassilis Kostakos<sup>†</sup>,
      Ting Dang<sup>†</sup>,
      Hong Jia<sup>§</sup>
    </div>

    <!-- 单位：先 Melbourne，再 Cambridge，最后 Auckland -->
    <div class="affiliations">
      <span><sup>†</sup> The University of Melbourne, Australia</span><br />
      <span><sup>‡</sup> The University of Cambridge, England</span><br />
      <span><sup>§</sup> The University of Auckland, New&nbsp;Zealand</span><br />

      <!-- equal contribution 说明，紧挨着单位 -->
      <span class="equal-note">
        * These authors contributed equally to this work.
      </span>
    </div>

      <div class="btn-row">
  <!-- GitHub 按钮 -->
  <a class="link-btn"
     href="https://github.com/xxue752-nz/Menta"
     target="_blank" rel="noopener noreferrer">
    <img src="11.png"
         alt="GitHub logo"
         class="btn-icon-img" />
    <span>GitHub</span>
  </a>

  <!-- Hugging Face 按钮 -->
  <a class="link-btn"
     href="https://huggingface.co/mHealthAI/Menta"
     target="_blank" rel="noopener noreferrer">
    <img src="12.png"
         alt="Hugging Face logo"
         class="btn-icon-img" />
    <span>Hugging&nbsp;Face</span>
  </a>
</div>

    </div>
  </header>

  <!-- ---------- 主体内容：所有 section ---------- -->
  <main class="content-wrapper">
    <div class="content-card">
      <!-- Abstract -->
      <section id="abstract" class="section">
        <h2>Abstract</h2>
        <p>
          Mental health conditions affect hundreds of millions of people worldwide, yet scalable and privacy-preserving early detection
          remains limited. While large language models (LLMs) have demonstrated strong performance in mental health prediction, their com-
          putational cost and deployment constraints hinder practical use on personal devices. Small language models (SLMs) offer a promising
          alternative, but the lack of standardized benchmarks has limited systematic evaluation of their effectiveness for social media–based
          mental health prediction. In this work, we introduce MentaBench, a unified benchmark for multi-task mental health prediction from
          social media text, covering six classification tasks spanning depression, stress, and suicidality. The benchmark evaluates model
          accuracy, robustness across datasets, and on-device deployability, enabling fair comparison between SLMs and LLMs. As a refer-
          ence system, we present Menta, an optimized SLM trained using a LoRA-based multi-task framework with balanced accuracy–oriented
          objectives. Across nine state-of-the-art SLMs and multiple LLM baselines, Menta achieves an average improvement of 15.2% over
          non–fine-tuned SLMs and outperforms 13B-parameter LLMs on several tasks while being approximately 3.25× smaller. We further
          demonstrate real-time, on-device inference on an iPhone 15 Pro Max using approximately 3GB of memory. By providing evaluations
          among baseline models, MentaBench introduces reproducible comparison and highlights the potential of SLMs for scalable, privacy-
          preserving mental health monitoring.
          <img src="3.png" alt="main" style="display:block;width:100%;max-width:850px;margin:20px auto 8px;border-radius:18px;"/>
        </p>
      </section>

      <!-- 1. Overview -->
      <section class="section">
        <h2>1. Overview</h2>
        <p>
          MentaBench is a unified benchmark for multi-task mental health prediction from social media text.
          It is designed to enable systematic and reproducible evaluation of small language models (SLMs)
          for early screening of stress, depression, and suicidality, with a focus on accuracy, robustness,
          and on-device deployability.
        </p>
        <p>
          The benchmark covers six Reddit-based classification tasks spanning stress detection, depression
          severity, suicidal ideation, and suicide risk categories. Alongside the benchmark, we provide
          Menta as a compact reference SLM to demonstrate strong multi-task performance and practical
          on-device feasibility.
        </p>
        <ul class="method-list">
          <li>Unified benchmark covering six mental health classification tasks.</li>
          <li>Evaluation of accuracy, robustness across datasets, and on-device deployability.</li>
          <li>Reference SLM (Menta) based on a 4B-parameter Qwen-style backbone.</li>
          <li>LoRA-based multi-task fine-tuning for efficient adaptation.</li>
          <li>Balanced-accuracy–aware optimization to handle imbalanced labels.</li>
        </ul>
      </section>

      <!-- 2. Model & Training -->
      <section id="model" class="section">
        <h2>2. Model and Training</h2>
        <p>
          As a reference system for MentaBench, Menta is built on top of a 4B-parameter transformer-based
          small language model and fine-tuned with parameter-efficient LoRA adapters for multi-task mental
          health prediction. The base model remains mostly frozen while LoRA layers capture task-specific
          adaptations.
        </p>

        <div class="badge-row">
          <span class="badge">4B SLM backbone</span>
          <span class="badge">LoRA adapters</span>
          <span class="badge">Multi-task training</span>
          <span class="badge">Balanced-accuracy loss</span>
        </div>

        <p>
          The training pipeline uses a shared transformer backbone with task-specific classification
          heads. LoRA adapters are inserted into attention projections (e.g., query and value
          matrices), enabling effective fine-tuning while updating only a small fraction of the total
          parameters.
        </p>
        <ul class="method-list">
  <li><strong>Parameter-efficient tuning.</strong> Only LoRA parameters and classifier heads are trained; base model weights are frozen, substantially reducing GPU memory requirements.</li>
  <li><strong>Task sampling.</strong> A task-level sampling strategy mitigates dataset size imbalance, preventing large datasets from dominating the multi-task objective.</li>
  <li><strong>Class imbalance handling.</strong> Class-weighted cross-entropy and a balanced-accuracy–aware term are combined to encourage robust performance on minority classes.</li>
  <li><strong>Joint optimization.</strong> All six tasks are optimized in a single training run, encouraging the model to share knowledge across stress, depression, and suicidality detection.</li>
</ul>

      </section>

      <!-- 3. Results -->
<section id="results" class="section">
  <h2>3. Results</h2>

  <!-- 3.1 Overall performance -->
  <h3>3.1 Overall performance compared with other small language models</h3>
  <p>
    Under the MentaBench evaluation, we compare Menta with several strong small language models that are used without
    mental health fine tuning, including Phi 4 Mini, StableLM and Falcon. On the six tasks
    that cover depression, stress and suicidality, Menta clearly moves the average level of
    performance. The average accuracy improves by about fifteen point two percentage points
    compared with the best setting among these models that are not fine tuned for mental
    health prediction.
  </p>
  <p>
    Figure&nbsp;6 shows accuracy and balanced accuracy scores for each task. The bars for Menta
    are usually the tallest in both plots. This means that Menta not only predicts correctly
    more often in general, but also treats minority labels such as severe depression and high
    suicide risk in a more balanced way. In other words, the model does not simply focus on
    the majority class, which is important for applications that care about safety.
  </p>

  <!-- 图 6：柱状图 -->
  <img
    src="f1.png"
    alt="Accuracy and balanced accuracy comparison for Phi 4 Mini, StableLM, Falcon and Menta on six tasks"
    style="display:block;width:100%;max-width:950px;margin:18px auto;border-radius:16px;"
  />

  <!-- 3.2 Multi task vs single task -->
  <h3>3.2 Multi task Menta compared with task specific variants</h3>
  <p>
    We then ask whether it is better to train one shared model or separate models for each
    task. For this purpose we fine tune six task specific variants, named Menta T1 to Menta T6,
    each one trained only on a single task. All of them use the same backbone and the same
    LoRA capacity as the general Menta model.
  </p>
  <p>
    The radar charts in Figure&nbsp;7 summarise the result. The red curve for the general Menta
    model tends to lie near the outer boundary on almost all axes. The curves for the task
    specific variants sometimes peak slightly higher on their own task, but they drop more on
    the remaining tasks and produce an irregular shape. Overall, the shared multi task model
    reaches higher average accuracy and balanced accuracy and shows a more even profile across
    all six tasks.
  </p>
  <p>
    This suggests that learning all tasks together helps the model capture common patterns in
    mental health language while still keeping good performance on each individual task. For
    deployment this is attractive, because one compact four billion parameter model is enough
    instead of six separate models.
  </p>

  <!-- 图 7：雷达图 -->
  <img
    src="f2.png"
    alt="Radar charts that show accuracy and balanced accuracy for the general Menta model and six task specific variants"
    style="display:block;width:100%;max-width:950px;margin:18px auto;border-radius:16px;"
  />

  <!-- 3.3 Case study -->
  <h3>3.3 Case study on depression severity</h3>
  <p>
    Finally we look at an example from the depression severity task in more detail. The post
    describes a sudden and very uncomfortable feeling on the tongue that has lasted for about
    twelve hours. The writer says that the tongue really starts to hurt and asks whether
    anyone else has ever felt something similar. In the dataset this case is labelled as
    minimum level of depression.
  </p>
  <p>
    In Figure&nbsp;10 the upper box contains the original post. The lower left part shows the
    prediction and reasoning of Menta, and the lower right part shows the output of Qwen three.
    Menta correctly assigns the minimum level. In its explanation it focuses on the fact that
    the main concern is a strange but local physical sensation, not sadness, hopelessness or
    other typical signs of a depressive episode. Menta reads the message as a request for
    medical reassurance and keeps the mental health severity low.
  </p>
  <p>
    Qwen three, in contrast, treats the same post as a case of severe distress. Its reasoning
    strongly reacts to phrases about intense pain and long duration and connects them directly
    with high anxiety and serious psychological problems. As a result it overestimates the
    level of depression. This case study illustrates that after domain specific training Menta
    is better at separating physical complaints from genuine mental health symptoms and follows
    the labelling rules of the dataset more closely.
  </p>

  <!-- 图 10：case study 图 -->
  <img
    src="f3.png"
    alt="Case study that compares Menta and Qwen three on a depression severity example with colour coded highlights"
    style="display:block;width:100%;max-width:950px;margin:18px auto;border-radius:16px;"
  />
</section>


      <!-- Datasets -->
      <section id="datasets" class="section">
        <h2>4. Datasets</h2>
        <p>
          MentaBench is built on four expert-annotated Reddit corpora, organized into six
          classification tasks that cover stress, depression, suicidal ideation, and suicide risk.
        </p>

        <table class="dataset-table">
          <thead>
            <tr>
              <th>Task</th>
              <th>Dataset</th>
              <th>Label type</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Task&nbsp;1</td>
              <td>Dreaddit</td>
              <td>Stress vs. non-stress</td>
            </tr>
            <tr>
              <td>Task&nbsp;2–3</td>
              <td>Depression severity dataset</td>
              <td>Binary depression + multi-level severity</td>
            </tr>
            <tr>
              <td>Task&nbsp;4</td>
              <td>SDCNL</td>
              <td>Suicidal ideation vs. non-ideation</td>
            </tr>
            <tr>
              <td>Task&nbsp;5–6</td>
              <td>CSSRS-based suicide risk dataset</td>
              <td>Binary risk + multi-level risk categories</td>
            </tr>
          </tbody>
        </table>

        <p class="highlight">
          <img src="4.png" alt="dataset" style="display:block;width:100%;max-width:850px;margin:20px auto 8px;border-radius:18px;"/>
        </p>
      </section>

      <!-- On-device Deployment -->
      <section id="deployment" class="section">
        <h2>5. On-Device Deployment</h2>
        <p>
          We demonstrate on-device deployment as part of MentaBench by running the reference model Menta
          using a lightweight inference stack with quantized weights. The goal is to enable privacy-preserving,
          real-time mental health screening directly on user devices without uploading raw text to remote servers.
        </p>
        <ul class="method-list">
  <li>Four billion parameter small language model with a Qwen style backbone.</li>
  <li>Six mental health classification tasks collected from expert annotated Reddit data.</li>
  <li>LoRA based multi task fine tuning for efficient adaptation.</li>
  <li>Balanced accuracy aware training objective that handles imbalanced labels.</li>
  <li>Real time on device deployment for privacy preserving mental health screening.</li>
</ul>

        <p class="highlight">
          <div class="device-row">
  <img src="device_demo1.jpg"
       alt="Menta mobile UI – model selection"
       class="device-shot" />

  <img src="device_demo2.jpg"
       alt="Menta mobile UI – task selection"
       class="device-shot" />

  <img src="device_demo3.jpg"
       alt="Menta mobile UI – sample count selection"
       class="device-shot" />
</div>

        </p>
      </section>

      <!-- Implementation / resources -->
      <section class="section">
        <h2>6. Implementation and Resources</h2>
        <p>
          The full benchmark pipeline, together with instructions for reproducing our
          experiments and running the reference model on mobile devices, is available in the GitHub repository.
          Pre-trained weights and configuration files for different quantization levels are hosted on
          Hugging&nbsp;Face.
        </p>
        <ul class="method-list">
          <li><strong>GitHub:</strong> end-to-end training, evaluation, and on-device demo code.</li>
          <li><strong>Hugging&nbsp;Face:</strong> model checkpoints and configuration files.</li>
          <li><strong>Mobile demo:</strong> a reference iOS application showing multi-task predictions for stress, depression, and suicidality from example posts.</li>
        </ul>
      </p>
      </section>
      <section id="bibtex" class="section">
  <h2>BibTeX</h2>
  <p>
    If you find <strong>MentaBench</strong> useful in your research, please cite it as:
  </p>

  <pre><code>@inproceedings{mentabench2026,
  title     = {MentaBench: A Multi-Task Benchmark for Mobile Mental Health Prediction with Small Language Models on Social Media Posts},
  author    = {Zhang, Tianyi and Xue, Xiangyuan and Ruan, Lingyan and Gao, Yan and D'Alfonso, Simon and Kostakos, Vassilis and Dang, Ting and Jia, Hong},
  booktitle = {ACM Conference Proceedings},
  year      = {2026},
  publisher = {ACM},
  doi       = {10.1145/nnnnnnn.nnnnnn}
}</code></pre>
</section>

    </div>
  </main>

  <footer>
    <span>© 2026 MentaBench Authors.</span>
    <span>Code and models for research use only. Please see the repository for license details.</span>
  </footer>

</body>
</html>



